page1

spark-sumbit --master yarn-cluster(deploy mode, yarn-cluster or yarn-client)\
--driver-core 2G\
--driver-memory 2G\
--num-executors 10\
--executor-memory 20G\
--executor-cores 10\
--conf spark.dynamicAllocation.minExecutors=5\
--conf spark.dynamicAllocation.maxExecutors=10\
--conf spark.dynamicAllocation.initialExecutors=7\
--class com.spark.com 
--jars jarfilename
-----------------------------Out-of-Memory---------------
Driver OOM- 
1. collect()
2. Broadcast join() if the file is too large. Default value of broadcast join is 10 MB
Executor OOM-
1. Yarn memory overhead - All the strings hashmap of program and objects are stored here
2. High concurrency - Assigning too much cores to executor
3. Small and Big partitions.

---------------------Spark Performanance Optimization-----------------------------------
1. Serialization - default is Java serialization, we can change it io kyro serialization for 10 times faster performanance.
2. API Selection
3. Broadcasting data
4. Cache() and Persist()
5. By Key operations - Use reduce by key instead group by key as group by key involve shuffle
6. File format selection
7. Garbage collection tuning - use verbose
8. Level of parallelism - Repartition() and coalesce()
-----------------------------Bucketing Formula in Hive------------------------
1. File size/Block size = 500 MB/128 MB = 4
2. optional---------Buckect size will always be in the power of 2 and greater than 4
3. 2*2*2, so that 8 will be the bucket size

-----------------------Hive Performanance Tuning-----------------------------
1. Execution engine - default is map reduce, set to Tez. set hive.execution.engine=tez ,2-4 times fast
2. Patitioning
3. Bucketing
4. De-Normalize data
5. Compression 
6. Map side join
7. Input file format
8. Parallel execution - set hive.exec.parallel=true
9. Vectorization
----------------------------Group by and Rank------------------------------------------------------
df1.groupBy("Name","Age","Salary","City").count().where("count>1").show()
val win=Window.partitionBy("Name").orderBy("Age","Salary")
df1.withColumn("row_num",row_number().over(win)).filter("row_num>1").show()
------------------------------Broadcast join and sortmerge join-----------------------------------------------------
println( spark.conf.get("spark.sql.autoBroadcastJoinThreshold").toInt/1024/1024)
BroadCast Join and SortMerge Join
df1.join(df2,df1("actor_id")<=>df2("city_id")).explain() //default is broadcast join,table size less than = 10 mb
import org.apache.spark.sql.functions.broadcast
df1.join(broadcast(df2),df1("actor_id")<=>df2("city_id")).explain()//Explicit broadcast join
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)//disable autobroadcast join
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", 20485760)
spark.conf.set("spark.sql.shuffle.partitions",3)
df1.join(df2,df1("actor_id")<=>df2("city_id")).explain()
-------------------------------------------withColumn and select----------------------------------------------------------------------------
println(df1.rdd.getNumPartitions)
import spark.implicits._
df1.select("Sales","Profit").filter("Sales>10").show()
df1.withColumn("Sales",df1("Sales")*10).show()
df1.select(col("Sales")*10 , col("Profit")*10).show()
val df2=df1.withColumn("Year",substring($"Order_Date",1,4).cast(IntegerType))
val df9 = df8.na.fill(115.0,Seq("Sales","Discount","Profit"))
val df10=df9.repartition(5)
-----------------------replace and cast---------------------------------------------------------------
val df2=df1.withColumn("Order Date",to_date(col("Order Date"),"dd-MMM-yy"))
val df3= df2.withColumn("Discount",regexp_replace(col("Discount"),"%",""))
val df3 = df2.withColumn("Discount",expr("replace(Discount, '%', '')"))
val df6=df5.withColumn("Profit",col("Profit").cast(FloatType))
-------------------------------------------------------------------------------------------

Page2------------------------------------------------------------------------------------------

rlike----------------
df2.withColumn(
  "Deposits",
  when(col("Deposits").rlike("^(1|0)"), "1.1").otherwise(col("Deposits"))
).show
df2.withColumn(
    "Deposits",
    when(
        col("Deposits").startsWith("0") || col("Deposits").startsWith("1"),
        "1.1"
    ).otherwise(
        col("Deposits")
    )
)
------------------------------------
df1
.select
 (
  (col("Sales")*10).as("Sales"),
  (col("Profit")*10).as("Profit")
)
.show()
replace and regexp/regex replace-----------------------------------------
Use replace for replacements of string patterns. regexp_replace is for replacing regex patterns, and $ has a special meaning in regex (end of string).
val sale2 = sale1.withColumn(
    "SaleAmount",
    expr("replace(SalesAmount, '$', '')")
) ----------------------------------------------------------

Imp_functions--------------------------------------
object readschema {
  @transient lazy val logger: Logger=Logger.getLogger(getClass.getName)
  def main(args: Array[String]):Unit={
    Logger.getLogger("org").setLevel(Level.ERROR)
    val spark=SparkSession.builder.master("local[*]").appName("schema").getOrCreate()
    //val schema="actor_id int,first_name string,last_name string,last_update timestamp"
    val df1=spark.read.option("header",true).option("inferSchema",true).csv("C:\\Users\\Sayyad Manzil\\Desktop\\duplicate.csv")
    val df2=spark.read.option("header",true).option("inferSchema",true).csv("E:\\Salman\\Hive-Data\\city.csv")

    /*df1.printSchema()
    df1.show(5)
    df2.printSchema()
    df2.show(5)*/
    //df1.groupBy("Name","Age","Salary","City").count().where("count>1").show()
    //val win=Window.partitionBy("Name").orderBy("Age","Salary")
    //df1.withColumn("row_num",row_number().over(win)).filter("row_num>1").show()
    //df1.withColumn("row_num",row_number().over(win)).filter("row_num>1").show()
    //df1.withColumn("rank",rank().over(win)).show()
    //df1.withColumn("rank",dense_rank().over(win)).show()
    //println( spark.conf.get("spark.sql.autoBroadcastJoinThreshold").toInt/1024/1024)
    //BroadCast Join and SortMerge Join
    //df1.join(df2,df1("actor_id")<=>df2("city_id")).explain() //default is broadcast join,table size less than = 10 mb
    import org.apache.spark.sql.functions.broadcast
    //df1.join(broadcast(df2),df1("actor_id")<=>df2("city_id")).explain()//Explicit broadcast join
    //spark.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)//disable autobroadcast join
    //spark.conf.set("spark.sql.autoBroadcastJoinThreshold", 20485760)
    //spark.conf.set("spark.sql.shuffle.partitions",3)
    //df1.join(df2,df1("actor_id")<=>df2("city_id")).explain()
    //df1.createOrReplaceTempView("table1")
    //spark.sql("select * from table1 limit 5").show()
    /*spark.sql("select Name,Salary,row_number() over(partition by Name order by Salary) as row_number," +
      "rank() over(partition by Name order by Salary) as rank," +
      "dense_rank() over(partition by Name order by Salary) as dense_rank from table1").show()*/
    import spark.implicits._
    
   //df1.select("City").withColumn("isNull_City", col("City").isNull).where("isNull_City = true").show
    df1.withColumn("isNull_City", col("City").isNull).where("isNull_City = true").show
----------------------------------------------------------------------------------------------------------------
page3----------------------------------------------------------------------------------------------------------
val df6=df5.withColumn("Profit",col("Profit").cast(FloatType)).
      withColumn("Sales",col("Sales").cast(FloatType)).
      withColumn("Discount",col("Discount").cast(FloatType))
----------------------------------------------------------------------------------------------------------

    //DataFrame operations that trigger shufflings are join(), union() and all aggregate functions.
    //spark.sql.shuffle.partitions which is by default set to 200.
    //val df7 = df6.groupBy("Category").count()
    //println(df7.rdd.getNumPartitions)

    //spark.conf.set("spark.sql.shuffle.partitions",100)
    //println(df6.groupBy("Category").count().rdd.partitions.length)

    //Spark submit - https://sparkbyexamples.com/spark/spark-submit-command/
---------------------------------------------------------------------------------------------------------------------

    /* Shuffle partition size
Based on your dataset size, number of cores, and memory, Spark shuffling can benefit or harm your jobs.
When you dealing with less amount of data, you should typically reduce the shuffle partitions otherwise you will
end up with many partitioned files with a fewer number of records in each partition. which results in running many
tasks with lesser data to process.

On other hand, when you have too much of data and having less number of partitions results in fewer longer running
tasks and some times you may also get out of memory error.

Getting a right size of the shuffle partition is always tricky and takes many runs with different value to achieve the
optimized number. This is one of the key property to look for when you have performance issues on Spark jobs. */
-------------------------------------------------------------------------------------------

    val df8=df6.withColumnRenamed("Customer Name","Customer_Name").
      withColumnRenamed("Number of Records","Number_of_Records").
      withColumnRenamed("Order Date","Order_Date").
      withColumnRenamed("Product Name","Product_Name")
----------------------------------------------------------------------------------------------------

    import spark.implicits._
    val df9 = df8.na.fill(115.0,Seq("Sales","Discount","Profit"))
    val df10=df9.repartition(5)
    df10.write.mode("overwrite").option("header",true).parquet("C:\\Users\\Sayyad Manzil\\Desktop\\Spark_Data\\superstore.parquet")
--------------------------------------------------------------------------------------------------------------------

    /*val old_columns = Seq("Customer Name","Number of Records","Order Date","Product Name")
    val new_columns = Seq("Customer_Name","Number_of_Records","Order_Date","Product_Name")
    val columnsList = old_columns.zip(new_columns).map(f=>{col(f._1).as(f._2)})
    val df8 = df6.select(columnsList:_*) */
-------------------------------------------------------------------------------------------------------


 //df1.select("Sales","Profit").filter("Sales>10").show()
    //df1.withColumn("Sales",df1("Sales")*10).show()
    //https://stackoverflow.com/questions/46004082/how-to-perform-arithmetic-operation-on-two-seperate-dataframes-in-apache-spark
    //df1.select(col("Sales")*10 , col("Profit")*10).show()
    //val df2=df1.withColumn("Year",substring($"Order_Date",1,4).cast(IntegerType))
    df1.printSchema()
-------------------------------------------------------------------------

    /*println(df1.rdd.partitions.length)
    val df2 = df1.repartition(6)
    val df3 = df2.coalesce(4)
    println(df2.rdd.partitions.length)
    println(df3.rdd.partitions.length)*/


--------------------------------------------------Group_by and Rank-----------------------------------------------------------------------------
 df1.groupBy("Name","Age","Salary","City").count().where("count>1").show()
    //val win=Window.partitionBy("Name").orderBy("Age","Salary")
    //df1.withColumn("row_num",row_number().over(win)).filter("row_num>1").show()
    //df1.withColumn("row_num",row_number().over(win)).filter("row_num>1").show()
    //df1.withColumn("rank",rank().over(win)).show()
    //df1.withColumn("rank",dense_rank().over(win)).show()
--------------------------------Broadcast and Sortmerge join-------sort merge join is normal shuffle join------------------------------------------------
//println( spark.conf.get("spark.sql.autoBroadcastJoinThreshold").toInt/1024/1024)
    //BroadCast Join and SortMerge Join
    //df1.join(df2,df1("actor_id")<=>df2("city_id")).explain() //default is broadcast join,table size less than = 10 mb
    import org.apache.spark.sql.functions.broadcast
    //df1.join(broadcast(df2),df1("actor_id")<=>df2("city_id")).explain()//Explicit broadcast join
    //spark.conf.set("spark.sql.autoBroadcastJoinThreshold", -1)//disable autobroadcast join
    //spark.conf.set("spark.sql.autoBroadcastJoinThreshold", 20485760)
    //spark.conf.set("spark.sql.shuffle.partitions",3)
    //df1.join(df2,df1("actor_id")<=>df2("city_id")).explain()
----------------------------------------------------------------------------------------------------------------------

page4 -----------Hive------------------
Hive add partition - ALTER TABLE film1 ADD PARTITION (ratings='R') location 's3://hivetables123/film/';
Load data from partitions - MSCK REPAIR TABLE table_name;(Athena)
Hive partitioning explain - https://www.javatpoint.com/partitioning-in-hive
Increamental_load - https://www.youtube.com/watch?v=B51yDF04xLw
File systems - https://www.youtube.com/watch?v=q8uqpwxaVZw

----------------------------------static Partitioning------------------------------------
hive> create table student (id int, name string, age int,  institute string)   
partitioned by (course string)  
row format delimited  
fields terminated by ',';  

hive> load data local inpath '/home/codegyani/hive/student_details1' into table student  
partition(course= "java");    

hive> load data local inpath '/home/codegyani/hive/student_details2' into table student  
partition(course= "hadoop");  

-------------------------------------Dynamic Partitioning-----------------------------------
Enable the dynamic partition by using the following commands: -
hive> set hive.exec.dynamic.partition=true;    
hive> set hive.exec.dynamic.partition.mode=nonstrict;  

Create a dummy table to store the data.
hive> create table stud_demo(id int, name string, age int, institute string, course string)   
row format delimited  
fields terminated by ',';  

Now, load the data into the table.
hive> load data local inpath '/home/codegyani/hive/student_details' into table stud_demo;  

Create a partition table by using the following command: -
hive> create table student_part (id int, name string, age int, institute string)   
partitioned by (course string)  
row format delimited  
fields terminated by ',';  

Now, insert the data of dummy table into the partition table.
hive> insert into student_part  
partition(course)  
select id, name, age, institute, course  
from stud_demo; 

Now, try to retrieve the data based on partitioned columns by using the following command: -
hive> select * from student_part where course= "java ";  

-------------------------------Bucketing in HIVE---------------------------------------------
The concept of bucketing is based on the hashing technique.
Here, modules of current column value and the number of required buckets is calculated (let say, F(x) % 3).
Now, based on the resulted value, the data is stored into the corresponding bucket.

First, select the database in which we want to create a table.
hive> use showbucket; 

Create a dummy table to store the data.
hive> create table emp_demo (Id int, Name string , Salary float)    
row format delimited    
fields terminated by ',' ;  

Now, load the data into the table.
hive> load data local inpath '/home/codegyani/hive/emp_details' into table emp_demo;  

Enable the bucketing by using the following command: -
hive> set hive.enforce.bucketing = true;  

Create a bucketing table by using the following command: -
hive> create table emp_bucket(Id int, Name string , Salary float)    
clustered by (Id) into 3 buckets  
row format delimited    
fields terminated by ',' ;    

Now, insert the data of dummy table into the bucketed table.
hive> insert overwrite table emp_bucket select * from emp_demo;    
--------------------------------------------------------------------------------
Create table from parquet or avro file format

CREATE TABLE avro_test 
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.avro.AvroSerDe' 
STORED AS AVRO 
TBLPROPERTIES ('avro.schema.url'='myHost/myAvroSchema.avsc'); 

CREATE EXTERNAL TABLE parquet_test 
STORED AS PARQUET 
LOCATION 'hdfs://myParquetFilesPath';

create table customer(id string,Branch string,City string,customer_type string,Gender string) 
create table product(id string,name string,price float,Quantity int,tax float,total float) 
create table rating(id string,order_date string,payment_mode string,cogs float,gmp float,gross_income float,rating float) 
create table sales_table(id string,Branch string,City string,customer_type string,Gender string,name string,price float,Quantity int,tax float,total float,order_date string,payment_mode string,cogs float,gmp float,gross_income float,rating float)
row format delimited
fields terminated by ','
load data local inpath '/home/sayyad/Downloads/Datasets/sales_table/sales_table.csv' into table sales_table

val customer=df1.select("id","Branch","City","customer_type","Gender")

    val product=df1.select("id","name","price","Quantity","tax","total")

    val rating=df1.select("id","Date","Payment","cogs","gmp","gross_income","Rating")

-----------------------------------------------------------
Hive window functions - https://stackoverflow.com/questions/55909029/windowing-function-in-hive



 







